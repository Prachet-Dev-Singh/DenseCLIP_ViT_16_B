# Model configuration
model:
  type: 'DenseCLIP'
  context_length: 77
  context_feature: 'attention'
  score_concat_index: -1 # changed from 3 to avoid error : score_concat_index 3 invalid. Score map not concatenated.
  text_head: False
  tau: 0.05  # Adjusted for better logit scaling
  token_embed_dim: 512
  text_dim: 512 #768 earlier
  clip_pretrained: '/home/22dcs005/DenseCLIP/segmentation/pretrained/ViT-B-16.pt' # Adjust path if needed based on where you run train_denseclip.py from

  backbone:
    type: 'CLIPVisionTransformer' # Change type
    input_resolution: 224     # Standard CLIP ViT input size (can sometimes be adapted)
    patch_size: 16            # For ViT-B/16
    width: 768                # Embedding dimension for ViT-B/16
    layers: 12                # Number of transformer layers for ViT-B/16
    heads: 12                 # Number of attention heads for ViT-B/16
    output_dim: 768
    out_indices: [7, 11] # Example indices - MAKE SURE THIS IS SET!

  text_encoder:
    type: 'CLIPTextEncoder'
    context_length: 77
    vocab_size: 49408
    transformer_width: 512
    transformer_heads: 8
    transformer_layers: 12
    embed_dim: 512 #768 earlier

  # neck:
  #   type: 'FPN'
  #   in_channels: [256, 512, 1024, 2048] # Channels from ResNet stages before projection
  #   out_channels: 256
  #   num_outs: 4 # Or 5 if using extra level

  # --- VVVVVV ADD NECK SECTION VVVVVV ---
  neck:
    type: 'ViTFeatureFusionNeck'      # <<< Use the name of your new neck class
    in_channels_list: [768, 768]      # <<< Channels matching ViT width for out_indices [7, 11]
    out_channels: 256                 # <<< Desired output channels of the neck (e.g., 256)
    inter_channels: 128               # <<< Optional: Internal channels for processing blocks (e.g., 128)
  # --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---

  decode_head:
    type: 'FPNHead'
    in_channels: 256 #768 earlier
    channels: 256
    num_classes: 19
    align_corners: False
    dropout_ratio: 0.1

# Data configuration
data:
  dataset_type: 'CityscapesDataset'
  path: 'data/cityscapes'
  classes: 19
  ignore_label: 255
  crop_size: [512, 1024]
  scale_range: [0.5, 2.0]
  norm_mean: [0.48145466, 0.4578275, 0.40821073]
  norm_std: [0.26862954, 0.26130258, 0.27577711]
  samples_per_gpu: 32
  workers_per_gpu: 32

# Training configuration
training:
  epochs: 100
  batch_size: 8
  workers: 8
  optimizer:
    type: 'AdamW'  # Changed to AdamW
    lr: 2.0e-05  # Reduced learning rate for better stability
    weight_decay: 0.01
  min_lr: 1.0e-06
  log_interval: 50
  seed: 42
  eval_interval: 1
  save_interval: 5
